{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data Science"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Making the training and test data sets"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as nm\nimport pandas as pd\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df=pd.read_csv('train.csv', index_col='Enter index column')\ntest_df=pd.read_csv('test.csv', index_col='Enter index column')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X=train_df.loc[:,'Replace by columns name from where to start except the answer column':].as_matrix().astype('float')\n\ny=train_df['Replace by answer column'].ravel()\n\nprint X.shape, y.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Splitting of datasets into train and test for input and output"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n\nprint X_train.shape, y_train.shape\n\nprint X_test.shape, y_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# PRINTING AVERAGE SURVIVAL IN TRAIN AND TEST\n\n# this output will be positive cases ie the people who survived\n# we want the two results to be nearly same, so that we know that both test and train are similar in nature \n\nprint 'Mean of result in train : {0:.3f}'.format(nm.mean(y_train))\n\nprint 'Mean of result in test : {0:.3f}'.format(nm.mean(y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Making a baseline model for comparision purpose"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.dummy import DummyClassifier\n\nmodel_dummy= DummyClassifier(strategy='most_frequent', random_state=0)\n\nmodel_dummy.fit(X_train,y_train)\n\n#giving inputs to the dummy algo or the baseline algo",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print 'Score for baseline model : {0: .2f}'.format(model_dummy.score(X_test, y_test))\n\n# in this the model will first get a prediction as output from dummy model or baseline model\n# all these outputs will then be compared to the y_test which the actual result, hence will give as an idea as to how accurately the algo is predicting by comparing the predicted o/p to the actual values\n\n#if the answer is 0.61, this means that 61% of times the baseline models predicts right\n\n# so without using ML, we are still getting .61 accuracy just by classification for predicitng survival or death",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Now making a performance matrix so that we can compare the scores of baseline with other algos such as regression\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n\n# all these are different kinds of performance matrixes ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# accuracy score\n\nprint 'accuracy for baseline model : {0: .2f}'.format(accuracy_score(y_test, model_dummy.predict(X_test)))\n\n# this output will give us the accuracy score of the baseline function\n\n# we have sent y_test is the actual result and model_dummy.predict(X_test) will send its predicted output\n# these both together will be compared to eachother and then accuracy will be shown",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# now showing the confusion matrix, the parameters is same for all the performance matrix\n\nprint 'confusion matrix for baseline model : \\n {0}'.format(confusion_matrix(y_test,model_dummy.predict(X_test)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# precision and recall scores\n\nprint 'precision score for baseline model : \\n {0}'.format(precision_score(y_test,model_dummy.predict(X_test)))\n\nprint 'recall score for baseline model : \\n {0}'.format(recall_score(y_test,model_dummy.predict(X_test)))\n\n# the warning is fine. Zero will answer to both",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Storing the output ie predicted values on a csv file( for submission or just reference)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_X= test_df.as_matrix().astype('float')\n\n# test_df is the dataframe for which we dont have answers\n\n# we will use baseline model to predict, so first we are converting it to matrix|",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# getting the predictions\n# predictions will get the predicted values from dummy algo ie baseline model\n\npredictions=model_dummy.predict(test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#now making a data frame that we will save/submit\n# we are using passengerID as index and their predicted value\n# so will have a file that will show whether that passenger ID person is dead or alive using prediction\n\n# remember this is prediction from baseline model. It can be wrong\n\ndf_submissions=pd.DataFrame({'Enter the Index': test_df.index,'Enter the name of the answers column': predictions})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_submissions.head()\n\n# showing the dataframe of predicted values that we created",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_submissions.to_csv('01_dummy.csv', index=False)\n\n#we are setting index=False so that no other columns is formed separately for the index\n\n# we can check after executing this line that a new file named 01_dummy.csv is created in the parent folder",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Logistic Regression Model\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\n\n#creating a model object\nmodel_lr_1=LogisticRegression(random_state=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#training the model\n\nmodel_lr_1.fit(X_train, y_train)\n\n# we can see a big message after this is run saying c=1 and various other things\n\n# these are various regularization features, we can change these values to get better results\n\n# we'll explore them after few steps under regularization steps",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# getting the model score on the test data\n# we'll get a score higher than the baseline model, so LR is atleast better than baseline\n\nprint 'score for logistic regression v 1 : {0:.2f}'.format(model_lr_1.score(X_test,y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print 'accuracy for LR model : {0: .2f}'.format(accuracy_score(y_test, model_lr_1.predict(X_test)))\n\nprint 'confusion matrix for LR model : \\n {0}'.format(confusion_matrix(y_test,model_lr_1.predict(X_test)))\n\nprint 'precision score for LR model : \\n {0}'.format(precision_score(y_test,model_lr_1.predict(X_test)))\n\nprint 'recall score for LR model : \\n {0}'.format(recall_score(y_test,model_lr_1.predict(X_test)))\n\n\n#We can see that everything improved from baseline model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Grid Search (Hyperparameter Optimization technique)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_lr=LogisticRegression(random_state=0)\n\nfrom sklearn.model_selection import GridSearchCV\n\n#using gridsearchcv function for hyperparameter optimization",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "parameters = {'C':[1.0,10.0,50,100,1000], 'penalty' : ['l1','l2']}\n\n# we are creatign a parameter dictionary to try during the grid operation\n\n# so we are trying 1.0, 10.0 are various other numbers for C and similarly L1 and L2 for penalty \n\nclf=GridSearchCV(model_lr,param_grid=parameters, cv=3)\n\n#first we mentioned the algo name on which we will be applying the optimization\n\n# param_grid will have all the different parameters that we want to try \n\n#cv=3 means perform 3 fold cross validation\n\n#clf is grid search object",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "clf.fit(X_train, y_train)\n\n#now here we are passing the training data into the grid search object\n\n#the object when it was created above, we already sent the algo name,so in this line we dont need to mention the algo name that is regression, the object knows it\n\n#so we only have to send the trianing and test data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "clf.best_params_\n\n#best_params is a function which will give us the best and optimized values of C, so we are getting C=1 and L1 as most optimized hyperparameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print 'best score :{0:.2f}'.format(clf.best_score_)\n\n#no significant difference observed, most advanced algo we get improvements",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#evalute model\n\nprint 'score for Logistic Regression version : {0:.2f}'.format(clf.score(X_test,y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Feature Normalization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import MinMaxScaler, StandardScaler",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#feature normalization\n\nscaler=MinMaxScaler()\n\nX_train_scaled=scaler.fit_transform(X_train)\n\n# this line is working in two parts, first part is sending X_train to fit_transform, this will fit the scaler and transform the scaled output ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train_scaled[:,0].min(),X_train_scaled[:,0].max()\n\n# this is givnig the minimum and maximum values of the scaled values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#normalization test data\n# this is scaling the tst data also\nX_test_scaled=scaler.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## feature standardization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler=StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_lr=LogisticRegression(random_state=0)\nparameters={'C':[1.0,10.0,50.0,100.0,1000.0],'penalty':['l1','l2']}\n\nclf=GridSearchCV(model_lr, param_grid=parameters,cv=3)\nclf.fit(X_train,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "clf.best_score_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#evaluation model\n\nprint 'score for logistic regression - v2 : {0:.2f}'.format(clf.score(X_test_scaled, y_test))\n\n# we can see that their is no imporvement with feature standardiaztion\n\n# this happens because standardized features dont have good affect on  LR due to some techincal reasons\n\n#but still we apply to see if we get any improvement",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Model persistence\n\n### Our work on LR is done, now we are trying to save the model so that we can directly use this model, hence we are saving the model, this is called model persistence"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pickle",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#creating a file and opening it in write mode\n\nmodel_file_pickle=open('lr_model.pkl','wb')\nscaler_file_pickle=open('scaler_model.pkl','wb')\n\n# we need scaler model to save the standardized scalers we created in the over standarization\n# so we created scaler_model.pkl also\n\n# wb stands for writting in binary mode",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pickle.dump(clf, model_file_pickle)\npickle.dump(scaler, scaler_file_pickle)\n\n# model_file_pickle is the object name of the file\n\n# clf is the object of the hyperparametatized logistic regression\n\n# that is ... clf=GridSearchCV(model_lr,param_grid=parameters, cv=3)\n\n# so clf is like the optimized LR model with best parameters in it\n\n# becasue grid search CV will return a LR model with parameters set to the most optimized values\n\n\n\n# dump function in used to write the model and scalar objects ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_file_pickle.close()\nscaler_file_pickle.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Loading the persistent file\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# now opening persisted files as read\n\n\nmodel_file_pickle=open('lr_model.pkl','r')\nscaler_file_pickle=open('scaler_model.pkl', 'r')\n\n#load files\n\nclf_loaded=pickle.load(model_file_pickle)\n\nscaler_loaded=pickle.load(scaler_file_pickle)\n\n#close files\n\nmodel_file_pickle.close()\nscaler_file_pickle.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "clf_loaded",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler_loaded",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# transform the test data using loaded scaler object\n\nX_test_scaled=scaler_loaded.transform(X_test)\n\n#calculate the score using loaded model object\n\nprint 'score for persisited logistc regression : {0:.2f}'.format(clf_loaded.score(X_test_scaled, y_test))\n\nprint 'score for persisited logistc regression non scaled: {0:.2f}'.format(clf_loaded.score(X_test, y_test))\n\n# so this perticular problem does not work well when using a scaled data set\n\n# also we can see that we saved the whole model and no need to retrain the model\n\n# we are just sending it the test data sets, and no train data set\n\n# this is persisted model, a model which is alread learnt and does not require training",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}